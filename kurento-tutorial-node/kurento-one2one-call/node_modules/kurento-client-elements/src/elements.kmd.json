{
  "name": "elements",
  "version": "6.18.1-dev",
  "kurentoVersion": "^6.18.1-dev",
  "imports": [
    {
      "name": "core",
      "version": "^6.18.1-dev",
      "mavenVersion": "[6.18.1-SNAPSHOT,7.0.0-SNAPSHOT)",
      "npmVersion": "Kurento/kurento-client-core-js"
    }
  ],
  "code": {
    "kmd": {
      "java": {
        "mavenGroupId": "org.kurento",
        "mavenArtifactId": "kms-api-elements",
        "mavenVersion": "6.18.1-SNAPSHOT"
      }
    },
    "api": {
      "java": {
        "packageName": "org.kurento.client",
        "mavenGroupId": "org.kurento",
        "mavenArtifactId": "kurento-client",
        "mavenVersion": "6.18.1-SNAPSHOT"
      },
      "js": {
        "nodeName": "kurento-client-elements",
        "npmDescription": "JavaScript Client API for Kurento Media Server",
        "npmGit": "Kurento/kurento-client-elements-js",
        "npmVersion": "Kurento/kurento-client-elements-js"
      }
    },
    "implementation": {
      "cppNamespace": "kurento",
      "lib": "libkmselements"
    }
  },
  "remoteClasses": [
    {
      "name": "AlphaBlending",
      "doc": "A :rom:cls:`Hub` that mixes the :rom:attr:`MediaType.AUDIO` stream of its connected sources and constructs one output with :rom:attr:`MediaType.VIDEO` streams of its connected sources into its sink",
      "extends": "Hub",
      "constructor": {
        "params": [
          {
            "name": "mediaPipeline",
            "doc": "the :rom:cls:`MediaPipeline` to which the dispatcher belongs",
            "type": "MediaPipeline"
          }
        ],
        "doc": "Create for the given pipeline"
      },
      "methods": [
        {
          "params": [
            {
              "name": "source",
              "doc": "The reference to the HubPort setting as master port",
              "type": "HubPort"
            },
            {
              "name": "zOrder",
              "doc": "The order in z to draw the master image",
              "type": "int"
            }
          ],
          "name": "setMaster",
          "doc": "Sets the source port that will be the master entry to the mixer"
        },
        {
          "params": [
            {
              "name": "relativeX",
              "doc": "The x position relative to the master port. Values from 0 to 1 are accepted. The value 0, indicates the coordinate 0 in the master image.",
              "type": "float"
            },
            {
              "name": "relativeY",
              "doc": "The y position relative to the master port. Values from 0 to 1 are accepted. The value 0, indicates the coordinate 0 in the master image.",
              "type": "float"
            },
            {
              "name": "zOrder",
              "doc": "The order in z to draw the images. The greatest value of z is in the top.",
              "type": "int"
            },
            {
              "name": "relativeWidth",
              "doc": "The image width relative to the master port width. Values from 0 to 1 are accepted.",
              "type": "float"
            },
            {
              "name": "relativeHeight",
              "doc": "The image height relative to the master port height. Values from 0 to 1 are accepted.",
              "type": "float"
            },
            {
              "name": "port",
              "doc": "The reference to the confingured port.",
              "type": "HubPort"
            }
          ],
          "name": "setPortProperties",
          "doc": "Configure the blending mode of one port."
        }
      ]
    },
    {
      "name": "Composite",
      "doc": "A :rom:cls:`Hub` that mixes the :rom:attr:`MediaType.AUDIO` stream of its connected sources and constructs a grid with the :rom:attr:`MediaType.VIDEO` streams of its connected sources into its sink",
      "extends": "Hub",
      "constructor": {
        "params": [
          {
            "name": "mediaPipeline",
            "doc": "the :rom:cls:`MediaPipeline` to which the dispatcher belongs",
            "type": "MediaPipeline"
          }
        ],
        "doc": "Create for the given pipeline"
      }
    },
    {
      "name": "Dispatcher",
      "doc": "A :rom:cls:`Hub` that allows routing between arbitrary port pairs",
      "extends": "Hub",
      "constructor": {
        "params": [
          {
            "name": "mediaPipeline",
            "doc": "the :rom:cls:`MediaPipeline` to which the dispatcher belongs",
            "type": "MediaPipeline"
          }
        ],
        "doc": "Create a :rom:cls:`Dispatcher` belonging to the given pipeline."
      },
      "methods": [
        {
          "params": [
            {
              "name": "source",
              "doc": "Source port to be connected",
              "type": "HubPort"
            },
            {
              "name": "sink",
              "doc": "Sink port to be connected",
              "type": "HubPort"
            }
          ],
          "name": "connect",
          "doc": "Connects each corresponding :rom:enum:`MediaType` of the given source port with the sink port."
        }
      ]
    },
    {
      "name": "DispatcherOneToMany",
      "doc": "A :rom:cls:`Hub` that sends a given source to all the connected sinks",
      "extends": "Hub",
      "constructor": {
        "params": [
          {
            "name": "mediaPipeline",
            "doc": "the :rom:cls:`MediaPipeline` to which the dispatcher belongs",
            "type": "MediaPipeline"
          }
        ],
        "doc": "Create a :rom:cls:`DispatcherOneToMany` belonging to the given pipeline."
      },
      "methods": [
        {
          "params": [
            {
              "name": "source",
              "doc": "source to be broadcasted",
              "type": "HubPort"
            }
          ],
          "name": "setSource",
          "doc": "Sets the source port that will be connected to the sinks of every :rom:cls:`HubPort` of the dispatcher"
        },
        {
          "params": [],
          "name": "removeSource",
          "doc": "Remove the source port and stop the media pipeline."
        }
      ]
    },
    {
      "name": "HttpEndpoint",
      "doc": "Endpoint that enables Kurento to work as an HTTP server, allowing peer HTTP clients to access media.",
      "abstract": true,
      "extends": "SessionEndpoint",
      "methods": [
        {
          "params": [],
          "return": {
            "type": "String",
            "doc": "The url as a String"
          },
          "name": "getUrl",
          "doc": "Obtains the URL associated to this endpoint"
        }
      ]
    },
    {
      "name": "HttpPostEndpoint",
      "doc": "An :rom:cls:`HttpPostEndpoint` contains SINK pads for AUDIO and VIDEO, which provide access to an HTTP file upload function\n\n   This type of endpoint provide unidirectional communications. Its :rom:cls:`MediaSources <MediaSource>` are accessed through the HTTP POST method.",
      "extends": "HttpEndpoint",
      "constructor": {
        "params": [
          {
            "name": "mediaPipeline",
            "doc": "the :rom:cls:`MediaPipeline` to which the endpoint belongs",
            "type": "MediaPipeline"
          },
          {
            "name": "disconnectionTimeout",
            "doc": "This is the time that an http endpoint will wait for a reconnection, in case an HTTP connection is lost.",
            "type": "int",
            "optional": true,
            "defaultValue": 2
          },
          {
            "name": "useEncodedMedia",
            "doc": "Feed the input media as-is to the Media Pipeline, instead of first decoding it.\n              <p>\n              When this property is not enabled, the input media gets always decoded into a raw format before being processed by the rest of the Media Pipeline; this is done to ensure that Kurento is able to keep track of lost keyframes among other quality-control measurements. Of course, having to decode the media has a cost in terms of CPU usage, but ensures that the output streaming will be robust and reliable.\n              </p>\n              <p>\n              When this property is enabled, the explained behavior gets disabled. Instead, The endpoint will provide any input media directly to the Media Pipeline, without prior decoding. Enabling this mode of operation could have a severe effect on stability, because lost video keyframes will not be regenerated; however, avoiding a full cycle of decoding and encoding can be very useful for certain applications, because it improves performance by greatly reducing the CPU processing load.\n              </p>\n              <p>\n              Keep in mind that if this property is enabled, the original source media MUST already have an encoding format which is compatible with the destination target. For example: given a pipeline which uses this endpoint to read a file and then streams it to a WebRTC browser such as Chrome, then the file must already be encoded with a VP8 or H.264 codec profile which Chrome is able to decode. Note that for this example, most browsers don't support ANY combination of H.264 encoding options; instead, they tend to support only a very specific subset of the codec features (also known as 'profiles').\n              </p>\n              <p>\n              We strongly recommend to avoid using this option, because correct behavior cannot be guaranteed.\n              </p>\n              ",
            "type": "boolean",
            "optional": true,
            "defaultValue": false
          }
        ],
        "doc": "Builder for the :rom:cls:`HttpPostEndpoint`."
      },
      "events": [
        "EndOfStream"
      ]
    },
    {
      "name": "Mixer",
      "doc": "A :rom:cls:`Hub` that allows routing of video between arbitrary port pairs and mixing of audio among several ports",
      "extends": "Hub",
      "constructor": {
        "params": [
          {
            "name": "mediaPipeline",
            "doc": "the :rom:cls:`MediaPipeline` to which the Mixer belongs",
            "type": "MediaPipeline"
          }
        ],
        "doc": "Create a :rom:cls:`Mixer` belonging to the given pipeline."
      },
      "methods": [
        {
          "params": [
            {
              "name": "media",
              "doc": "The sort of media stream to be connected",
              "type": "MediaType"
            },
            {
              "name": "source",
              "doc": "Source port to be connected",
              "type": "HubPort"
            },
            {
              "name": "sink",
              "doc": "Sink port to be connected",
              "type": "HubPort"
            }
          ],
          "name": "connect",
          "doc": "Connects each corresponding :rom:enum:`MediaType` of the given source port with the sink port."
        },
        {
          "params": [
            {
              "name": "media",
              "doc": "The sort of media stream to be disconnected",
              "type": "MediaType"
            },
            {
              "name": "source",
              "doc": "Audio source port to be disconnected",
              "type": "HubPort"
            },
            {
              "name": "sink",
              "doc": "Audio sink port to be disconnected",
              "type": "HubPort"
            }
          ],
          "name": "disconnect",
          "doc": "Disonnects each corresponding :rom:enum:`MediaType` of the given source port from the sink port."
        }
      ]
    },
    {
      "name": "PlayerEndpoint",
      "doc": "Retrieves content from external sources.\n<p>\n  PlayerEndpoint will access the given resource, read all available data, and\n  inject it into Kurento. Once this is is done, the injected video or audio\n  will be available for passing through any other Filter or Endpoint to which\n  the PlayerEndpoint gets connected.\n</p>\n<p>\n  The source can provide either seekable or non-seekable media; this will\n  dictate whether the PlayerEndpoint is able (or not) to seek through the file,\n  for example to jump to any given timestamp.\n</p>\n<p>The <strong>Source URI</strong> supports these formats:</p>\n<ul>\n  <li>\n    File: A file path that will be read from the local file system. Example:\n    <ul>\n      <li><code>file:///path/to/file</code></li>\n    </ul>\n  </li>\n  <li>\n    HTTP: Any file available in an HTTP server. Examples:\n    <ul>\n      <li><code>http(s)://{server-ip}/path/to/file</code></li>\n      <li>\n        <code>\n          http(s)://{username}:{password}@{server-ip}:{server-port}/path/to/file\n        </code>\n      </li>\n    </ul>\n  </li>\n  <li>\n    RTSP: Typically used to capture a feed from an IP Camera. Examples:\n    <ul>\n      <li><code>rtsp://{server-ip}</code></li>\n      <li>\n        <code>\n          rtsp://{username}:{password}@{server-ip}:{server-port}/path/to/file\n        </code>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <strong>\n      NOTE (for current versions of Kurento 6.x): special characters are not\n      supported in <code>{username}</code> or <code>{password}</code>.\n    </strong>\n    This means that <code>{username}</code> cannot contain colons\n    (<code>:</code>), and <code>{password}</code> cannot contain 'at' signs\n    (<code>@</code>). This is a limitation of GStreamer 1.8 (the underlying\n    media framework behind Kurento), and is already fixed in newer versions\n    (which the upcoming Kurento 7.x will use).\n  </li>\n  <li>\n    <strong>\n      NOTE (for upcoming Kurento 7.x): special characters in\n      <code>{username}</code> or <code>{password}</code> must be url-encoded.\n    </strong>\n    This means that colons (<code>:</code>) should be replaced with\n    <code>%3A</code>, and 'at' signs (<code>@</code>) should be replaced with\n    <code>%40</code>.\n  </li>\n</ul>\n<p>\n  Note that\n  <strong> PlayerEndpoint requires read permissions to the source </strong>\n  ; otherwise, the media server won't be able to retrieve any data, and an\n  :rom:evt:`Error` will be fired. Make sure your application subscribes to this\n  event, otherwise troubleshooting issues will be difficult.\n</p>\n\n<p>The list of valid operations is:</p>\n<ul>\n  <li>\n    <strong><code>play</code></strong>\n    : Starts streaming media. If invoked after pause, it will resume playback.\n  </li>\n  <li>\n    <strong><code>stop</code></strong>\n    : Stops streaming media. If play is invoked afterwards, the file will be\n    streamed from the beginning.\n  </li>\n  <li>\n    <strong><code>pause</code></strong>\n    : Pauses media streaming. Play must be invoked in order to resume playback.\n  </li>\n  <li>\n    <strong><code>seek</code></strong>\n    : If the source supports seeking to a different time position, then the\n    PlayerEndpoint can:\n    <ul>\n      <li>\n        <strong><code>setPosition</code></strong>\n        : Allows to set the position in the file.\n      </li>\n      <li>\n        <strong><code>getPosition</code></strong>\n        : Returns the current position being streamed.\n      </li>\n    </ul>\n  </li>\n</ul>\n<h2>Events fired</h2>\n<ul>\n  <li>\n    <strong>EndOfStreamEvent</strong>: If the file is streamed completely.\n  </li>\n</ul>\n      ",
      "extends": "UriEndpoint",
      "constructor": {
        "params": [
          {
            "name": "mediaPipeline",
            "doc": "The :rom:cls:`MediaPipeline` this PlayerEndpoint belongs to.",
            "type": "MediaPipeline"
          },
          {
            "name": "uri",
            "doc": "URI pointing to the video. It has to be accessible to the KMS process.\n              <ul>\n                <li>Local resources: The user running the Kurento Media Server must have read permission over the file.</li>\n                <li>Remote resources: Must be accessible from the server where the media server is running.</li>\n              </ul>",
            "type": "String"
          },
          {
            "name": "useEncodedMedia",
            "doc": "Feed an encoded media as-is to the Media Pipeline, instead of first decoding it.\n              <p>\n                This property is disabled by default. The input media gets always decoded into\n                a raw format upon receiving it, before being processed by the rest of the\n                Media Pipeline. This is done to ensure that Kurento is able to keep track of\n                lost keyframes among other quality-control measurements. Of course, having to\n                decode the media has a cost in terms of CPU usage, but ensures that the output\n                streaming will be more robust and reliable.\n              </p>\n              <p>\n                When this property is enabled, Kurento simply passes the encoded media as-is\n                to the rest of the Media Pipeline, without decoding. Enabling this mode of\n                operation could have a severe effect on stability, because lost video\n                keyframes will not be regenerated; however, not having to encode the video\n                greatly reduces the CPU load.\n              </p>\n              <p>\n                Keep in mind that if this property is enabled, the original source media MUST\n                already be in a format that is compatible with the destination target. For\n                example: Given a Pipeline that reads a file and then streams it to a WebRTC\n                browser such as Chrome, the file must already be encoded with a VP8 or H.264\n                codec profile, which Chrome is able to decode.\n              </p>\n              <p>\n                Of special note is that you cannot feed any random combination of H.264\n                encoding options to a web browser; instead, they tend to support only a very\n                specific subset of the codec features (also known as 'profiles'). The most\n                compatible config for H.264 is\n                <strong>Constrained Baseline profile, level 3.1.</strong>\n              </p>\n              <p>Code examples:</p>\n              <pre><code>\n                # Java\n                PlayerEndpoint player = new PlayerEndpoint\n                  .Builder(pipeline, 'rtsp://localhost:5000/video')\n                  .useEncodedMedia()\n                  .build();\n              </code></pre>\n              <pre><code>\n                # JavaScript\n                let player = await pipeline.create('PlayerEndpoint', {\n                  uri: 'rtsp://localhost:5000/video',\n                  useEncodedMedia: true,\n                });\n              </code></pre>\n              ",
            "type": "boolean",
            "optional": true,
            "defaultValue": false
          },
          {
            "name": "networkCache",
            "doc": "RTSP buffer length.\n<p>\n  When receiving media from an RTSP source, the streamed video can suffer spikes\n  or stuttering, caused by hardware or network issues. Having a reception buffer\n  helps alleviate these problems, because it smoothes the stream of incoming\n  data to the receiving endpoint.\n</p>\n<p>\n  Finding a buffer length that works best for your connection might take some\n  tweaking, which can be done with this optional property. Note that a longer\n  buffer will be able to fix bigger network spikes, but at the cost of\n  introducing more latency to the media playback.\n</p>\n<ul>\n  <li>Unit: milliseconds.</li>\n  <li>Default: 2000.</li>\n</ul>\n              ",
            "type": "int",
            "optional": true,
            "defaultValue": 2000
          }
        ],
        "doc": "Create a PlayerEndpoint"
      },
      "properties": [
        {
          "name": "videoInfo",
          "doc": "Returns info about the source being played",
          "type": "VideoInfo",
          "readOnly": true
        },
        {
          "name": "elementGstreamerDot",
          "doc": "Returns the GStreamer DOT string for this element's private pipeline",
          "type": "String",
          "readOnly": true
        },
        {
          "name": "position",
          "doc": "Get or set the actual position of the video in ms. .. note:: Setting the position only works for seekable videos",
          "type": "int64"
        }
      ],
      "methods": [
        {
          "params": [],
          "name": "play",
          "doc": "Starts reproducing the media, sending it to the :rom:cls:`MediaSource`. If the endpoint\n\n          has been connected to other endpoints, those will start receiving media."
        }
      ],
      "events": [
        "EndOfStream"
      ]
    },
    {
      "name": "RecorderEndpoint",
      "doc": "Provides functionality to store media contents.\n<p>\n  RecorderEndpoint can store media into local files or send it to a remote\n  network storage. When another :rom:cls:`MediaElement` is connected to a\n  RecorderEndpoint, the media coming from the former will be muxed into\n  the selected recording format and stored in the designated location.\n</p>\n<p>\n  These parameters must be provided to create a RecorderEndpoint, and they\n  cannot be changed afterwards:\n</p>\n<ul>\n  <li>\n    <strong>Destination URI</strong>, where media will be stored. These formats\n    are supported:\n    <ul>\n      <li>\n        File: A file path that will be written into the local file system.\n        Example:\n        <ul>\n          <li><code>file:///path/to/file</code></li>\n        </ul>\n      </li>\n      <li>\n        HTTP: A POST request will be used against a remote server. The server\n        must support using the <i>chunked</i> encoding mode (HTTP header\n        <code>Transfer-Encoding: chunked</code>). Examples:\n        <ul>\n          <li><code>http(s)://{server-ip}/path/to/file</code></li>\n          <li>\n            <code>\n              http(s)://{username}:{password}@{server-ip}:{server-port}/path/to/file\n            </code>\n          </li>\n        </ul>\n      </li>\n      <li>\n        Relative URIs (with no schema) are supported. They are completed by\n        prepending a default URI defined by property <i>defaultPath</i>. This\n        property is defined in the configuration file\n        <i>/etc/kurento/modules/kurento/UriEndpoint.conf.ini</i>, and the\n        default value is <code>file:///var/lib/kurento/</code>\n      </li>\n      <li>\n        <strong>\n          NOTE (for current versions of Kurento 6.x): special characters are not\n          supported in <code>{username}</code> or <code>{password}</code>.\n        </strong>\n        This means that <code>{username}</code> cannot contain colons\n        (<code>:</code>), and <code>{password}</code> cannot contain 'at' signs\n        (<code>@</code>). This is a limitation of GStreamer 1.8 (the underlying\n        media framework behind Kurento), and is already fixed in newer versions\n        (which the upcoming Kurento 7.x will use).\n      </li>\n      <li>\n        <strong>\n          NOTE (for upcoming Kurento 7.x): special characters in\n          <code>{username}</code> or <code>{password}</code> must be\n          url-encoded.\n        </strong>\n        This means that colons (<code>:</code>) should be replaced with\n        '<code>%3A</code>', and 'at' signs (<code>@</code>) should be replaced\n        with '<code>%40</code>'.\n      </li>\n    </ul>\n  </li>\n  <li>\n    <strong>Media Profile</strong> (:rom:enum:`MediaProfileSpecType`), which\n    determines the video and audio encoding. See below for more details.\n  </li>\n  <li>\n    <strong>EndOfStream</strong> (optional), a parameter that dictates if the\n    recording should be automatically stopped once the EOS event is detected.\n  </li>\n</ul>\n<p>\n  Note that\n  <strong>\n    RecorderEndpoint requires write permissions to the destination\n  </strong>\n  ; otherwise, the media server won't be able to store any information, and an\n  :rom:evt:`Error` will be fired. Make sure your application subscribes to this\n  event, otherwise troubleshooting issues will be difficult.\n</p>\n<ul>\n  <li>\n    To write local files (if you use <code>file://</code>), the system user that\n    is owner of the media server process needs to have write permissions for the\n    requested path. By default, this user is named '<code>kurento</code>'.\n  </li>\n  <li>\n    To record through HTTP, the remote server must be accessible through the\n    network, and also have the correct write permissions for the destination\n    path.\n  </li>\n</ul>\n<p>\n  Recording will start as soon as the user invokes the\n  <code>record()</code> method. The recorder will then store, in the location\n  indicated, the media that the source is sending to the endpoint. If no media\n  is being received, or no endpoint has been connected, then the destination\n  will be empty. The recorder starts storing information into the file as soon\n  as it gets it.\n</p>\n<p>\n  <strong>Recording must be stopped</strong> when no more data should be stored.\n  This is done with the <code>stopAndWait()</code> method, which blocks and\n  returns only after all the information was stored correctly.\n</p>\n<p>\n  The source endpoint can be hot-swapped while the recording is taking place.\n  The recorded file will then contain different feeds. When switching video\n  sources, if the new video has different size, the recorder will retain the\n  size of the previous source. If the source is disconnected, the last frame\n  recorded will be shown for the duration of the disconnection, or until the\n  recording is stopped.\n</p>\n<p>\n  <strong>\n    NOTE: It is recommended to start recording only after media arrives.\n  </strong>\n  For this, you may use the <code>MediaFlowInStateChanged</code> and\n  <code>MediaFlowOutStateChanged</code>\n  events of your endpoints, and synchronize the recording with the moment media\n  comes into the Recorder.\n</p>\n<p>\n  <strong>\n    WARNING: All connected media types must be flowing to the RecorderEndpoint.\n  </strong>\n  If you used the default <code>connect()</code> method, it will assume both\n  AUDIO and VIDEO. Failing to provide both kinds of media will result in the\n  RecorderEndpoint creating an empty file and buffering indefinitely; the\n  recorder waits until all kinds of media start arriving, in order to\n  synchronize them appropriately.<br>\n  For audio-only or video-only recordings, make sure to use the correct,\n  media-specific variant of the <code>connect()</code> method.\n</p>\n<p>\n  For example:\n</p>\n<ol>\n  <li>\n    When a web browser's video arrives to Kurento via WebRTC, your\n    WebRtcEndpoint will emit a <code>MediaFlowOutStateChanged</code> event.\n  </li>\n  <li>\n    When video starts flowing from the WebRtcEndpoint to the RecorderEndpoint,\n    the RecorderEndpoint will emit a <code>MediaFlowInStateChanged</code> event.\n    You should start recording at this point.\n  </li>\n  <li>\n    You should only start recording when RecorderEndpoint has notified a\n    <code>MediaFlowInStateChanged</code> for ALL streams. So, if you record\n    AUDIO+VIDEO, your application must receive a\n    <code>MediaFlowInStateChanged</code> event for audio, and another\n    <code>MediaFlowInStateChanged</code> event for video.\n  </li>\n</ol>\n      ",
      "extends": "UriEndpoint",
      "constructor": {
        "params": [
          {
            "name": "mediaPipeline",
            "doc": "the :rom:cls:`MediaPipeline` to which the endpoint belongs",
            "type": "MediaPipeline"
          },
          {
            "name": "uri",
            "doc": "URI where the recording will be stored. It must be accessible from the media server process itself:\n              <ul>\n                <li>Local server resources: The user running the Kurento Media Server must have write permission over the file.</li>\n                <li>Network resources: Must be accessible from the network where the media server is running.</li>\n              </ul>",
            "type": "String"
          },
          {
            "name": "mediaProfile",
            "doc": "Selects the media format used for recording.\n<p>\n  The media profile allows you to specify which codecs and media container will\n  be used for the recordings. This is currently the only way available to tell\n  Kurento about which codecs should be used.\n</p>\n<p>\n  Watch out for these important remarks:\n</p>\n<ul>\n  <li>\n    If the format of incoming media differs from the recording profile, media\n    will need to be transcoded. Transcoding always incurs in noticeable CPU\n    load, so it is always good trying to avoid it. For instance, if a\n    VP8-encoded video (from WebRTC) is recorded with an MP4 recording profile\n    (which means H.264 encoding), the video needs to be transcoded from VP8 to\n    H.264. On the other hand, recording with the WEBM profile would allow to\n    store the video as-is with its VP8 encoding.\n  </li>\n  <li>\n    If you intend to record audio-only or video-only media, select the\n    appropriate <code>_AUDIO_ONLY</code> or <code>_VIDEO_ONLY</code> profile.\n    For example, to record a WebRTC screen capture (as obtained from a web\n    browser's call to <code>MediaDevices.getDisplayMedia()</code>), choose\n    <code>WEBM_VIDEO_ONLY</code> instead of just <code>WEBM</code>.\n  </li>\n</ul>\n              ",
            "type": "MediaProfileSpecType",
            "optional": true,
            "defaultValue": "WEBM"
          },
          {
            "name": "stopOnEndOfStream",
            "doc": "Forces the recorder endpoint to finish processing data when an End Of Stream (EOS) is detected in the stream",
            "type": "boolean",
            "optional": true,
            "defaultValue": false
          }
        ],
        "doc": "Builder for the :rom:cls:`RecorderEndpoint`"
      },
      "methods": [
        {
          "params": [],
          "name": "record",
          "doc": "Starts storing media received through the sink pad."
        },
        {
          "params": [],
          "name": "stopAndWait",
          "doc": "Stops recording and does not return until all the content has been written to the selected uri. This can cause timeouts on some clients if there is too much content to write, or the transport is slow"
        }
      ],
      "events": [
        "Recording",
        "Paused",
        "Stopped"
      ]
    },
    {
      "name": "RtpEndpoint",
      "doc": "Endpoint that provides bidirectional content delivery capabilities with remote networked peers through RTP or SRTP protocol. An :rom:cls:`RtpEndpoint` contains paired sink and source :rom:cls:`MediaPad` for audio and video. This endpoint inherits from :rom:cls:`BaseRtpEndpoint`.\n      </p>\n      <p>\n      In order to establish an RTP/SRTP communication, peers engage in an SDP negotiation process, where one of the peers (the offerer) sends an offer, while the other peer (the offeree) responds with an answer. This endpoint can function in both situations\n      <ul style='list-style-type:circle'>\n        <li>\n          As offerer: The negotiation process is initiated by the media server\n          <ul>\n            <li>KMS generates the SDP offer through the generateOffer method. This offer must then be sent to the remote peer (the offeree) through the signaling channel, for processing.</li>\n            <li>The remote peer process the Offer, and generates an Answer to this offer. The Answer is sent back to the media server.</li>\n            <li>Upon receiving the Answer, the endpoint must invoke the processAnswer method.</li>\n          </ul>\n        </li>\n        <li>\n          As offeree: The negotiation process is initiated by the remote peer\n          <ul>\n            <li>The remote peer, acting as offerer, generates an SDP offer and sends it to the WebRTC endpoint in Kurento.</li>\n            <li>The endpoint will process the Offer invoking the processOffer method. The result of this method will be a string, containing an SDP Answer.</li>\n            <li>The SDP Answer must be sent back to the offerer, so it can be processed.</li>\n          </ul>\n        </li>\n      </ul>\n      </p>\n      <p>\n      In case of unidirectional connections (i.e. only one peer is going to send media), the process is more simple, as only the emitter needs to process an SDP. On top of the information about media codecs and types, the SDP must contain the IP of the remote peer, and the port where it will be listening. This way, the SDP can be mangled without needing to go through the exchange process, as the receiving peer does not need to process any answer.\n      </p>\n      <p>\n      The user can set some bandwidth limits that will be used during the negotiation process.\n      The default bandwidth range of the endpoint is 100kbps-500kbps, but it can be changed separately for input/output directions and for audio/video streams.\n      <ul style='list-style-type:circle'>\n        <li>\n          Input bandwidth control mechanism: Configuration interval used to inform remote peer the range of bitrates that can be pushed into this RtpEndpoint object. These values are announced in the SDP.\n          <ul>\n            <li>\n              setMaxVideoRecvBandwidth: sets Max bitrate limits expected for received video stream.\n            </li>\n            <li>\n              setMaxAudioRecvBandwidth: sets Max bitrate limits expected for received audio stream.\n            </li>\n          </ul>\n        </li>\n        <li>\n          Output bandwidth control mechanism: Configuration interval used to control bitrate of the output video stream sent to remote peer. Remote peers can also announce bandwidth limitation in their SDPs (through the b=<modifier>:<value> tag). Kurento will always enforce bitrate limitations specified by the remote peer over internal configurations.\n          <ul>\n            <li>\n              setMaxVideoSendBandwidth: sets Max bitrate limits for video sent to remote peer.\n            </li>\n            <li>\n              setMinVideoSendBandwidth: sets Min bitrate limits for audio sent to remote peer.\n            </li>\n          </ul>\n        </li>\n      </ul>\n      All bandwidth control parameters must be changed before the SDP negotiation takes place, and can't be modified afterwards.\n      TODO: What happens if the b=as tag form the SDP has a lower value than the one set in setMinVideoSendBandwidth?\n      </p>\n      <p>\n      Take into consideration that setting a too high upper limit for the output bandwidth can be a reason for the local network connection to be overflooded.\n      </p>\n      ",
      "extends": "BaseRtpEndpoint",
      "constructor": {
        "params": [
          {
            "name": "mediaPipeline",
            "doc": "the :rom:cls:`MediaPipeline` to which the endpoint belongs",
            "type": "MediaPipeline"
          },
          {
            "name": "crypto",
            "doc": "SDES-type param. If present, this parameter indicates that the communication will be encrypted. By default no encryption is used.",
            "type": "SDES",
            "optional": true,
            "defaultValue": {}
          },
          {
            "name": "useIpv6",
            "doc": "This configures the endpoint to use IPv6 instead of IPv4.",
            "type": "boolean",
            "optional": true,
            "defaultValue": false
          }
        ],
        "doc": "Builder for the :rom:cls:`RtpEndpoint`"
      },
      "events": [
        "OnKeySoftLimit"
      ]
    },
    {
      "name": "WebRtcEndpoint",
      "doc": "Control interface for Kurento WebRTC endpoint.\n<p>\n  This endpoint is one side of a peer-to-peer WebRTC communication, being the\n  other peer a WebRTC capable browser -using the RTCPeerConnection API-, a\n  native WebRTC app or even another Kurento Media Server.\n</p>\n<p>\n  In order to establish a WebRTC communication, peers engage in an SDP\n  negotiation process, where one of the peers (the offerer) sends an offer,\n  while the other peer (the offeree) responds with an answer. This endpoint can\n  function in both situations\n</p>\n<ul>\n  <li>\n    As offerer: The negotiation process is initiated by the media server\n    <ul>\n      <li>\n        KMS generates the SDP offer through the\n        <code>generateOffer</code> method. This <i>offer</i> must then be sent\n        to the remote peer (the offeree) through the signaling channel, for\n        processing.\n      </li>\n      <li>\n        The remote peer processes the <i>offer</i>, and generates an\n        <i>answer</i>. The <i>answer</i> is sent back to the media server.\n      </li>\n      <li>\n        Upon receiving the <i>answer</i>, the endpoint must invoke the\n        <code>processAnswer</code> method.\n      </li>\n    </ul>\n  </li>\n  <li>\n    As offeree: The negotiation process is initiated by the remote peer\n    <ul>\n      <li>\n        The remote peer, acting as offerer, generates an SDP <i>offer</i> and\n        sends it to the WebRTC endpoint in Kurento.\n      </li>\n      <li>\n        The endpoint will process the <i>offer</i> invoking the\n        <code>processOffer</code> method. The result of this method will be a\n        string, containing an SDP <i>answer</i>.\n      </li>\n      <li>\n        The SDP <i>answer</i> must be sent back to the offerer, so it can be\n        processed.\n      </li>\n    </ul>\n  </li>\n</ul>\n<p>\n  SDPs are sent without ICE candidates, following the Trickle ICE optimization.\n  Once the SDP negotiation is completed, both peers proceed with the ICE\n  discovery process, intended to set up a bidirectional media connection. During\n  this process, each peer\n</p>\n<ul>\n  <li>\n    Discovers ICE candidates for itself, containing pairs of IPs and ports.\n  </li>\n  <li>\n    ICE candidates are sent via the signaling channel as they are discovered, to\n    the remote peer for probing.\n  </li>\n  <li>\n    ICE connectivity checks are run as soon as the new candidate description,\n    from the remote peer, is available.\n  </li>\n</ul>\n<p>\n  Once a suitable pair of candidates (one for each peer) is discovered, the\n  media session can start. The harvesting process in Kurento, begins with the\n  invocation of the <code>gatherCandidates</code> method. Since the whole\n  Trickle ICE purpose is to speed-up connectivity, candidates are generated\n  asynchronously. Therefore, in order to capture the candidates, the user must\n  subscribe to the event <code>IceCandidateFound</code>. It is important that\n  the event listener is bound before invoking <code>gatherCandidates</code>,\n  otherwise a suitable candidate might be lost, and connection might not be\n  established.\n</p>\n<p>\n  It's important to keep in mind that WebRTC connection is an asynchronous\n  process, when designing interactions between different MediaElements. For\n  example, it would be pointless to start recording before media is flowing. In\n  order to be notified of state changes, the application can subscribe to events\n  generated by the WebRtcEndpoint. Following is a full list of events generated\n  by WebRtcEndpoint:\n</p>\n<ul>\n  <li>\n    <code>IceComponentStateChange</code>: This event informs only about changes\n    in the ICE connection state. Possible values are:\n    <ul>\n      <li><code>DISCONNECTED</code>: No activity scheduled</li>\n      <li><code>GATHERING</code>: Gathering local candidates</li>\n      <li><code>CONNECTING</code>: Establishing connectivity</li>\n      <li><code>CONNECTED</code>: At least one working candidate pair</li>\n      <li>\n        <code>READY</code>: ICE concluded, candidate pair selection is now final\n      </li>\n      <li>\n        <code>FAILED</code>: Connectivity checks have been completed, but media\n        connection was not established\n      </li>\n    </ul>\n    The transitions between states are covered in RFC5245. It could be said that\n    it's network-only, as it only takes into account the state of the network\n    connection, ignoring other higher level stuff, like DTLS handshake, RTCP\n    flow, etc. This implies that, while the component state is\n    <code>CONNECTED</code>, there might be no media flowing between the peers.\n    This makes this event useful only to receive low-level information about the\n    connection between peers. Even more, while other events might leave a\n    graceful period of time before firing, this event fires immediately after\n    the state change is detected.\n  </li>\n  <li>\n    <code>IceCandidateFound</code>: Raised when a new candidate is discovered.\n    ICE candidates must be sent to the remote peer of the connection. Failing to\n    do so for some or all of the candidates might render the connection\n    unusable.\n  </li>\n  <li>\n    <code>IceGatheringDone</code>: Raised when the ICE gathering process is\n    completed. This means that all candidates have already been discovered.\n  </li>\n  <li>\n    <code>NewCandidatePairSelected</code>: Raised when a new ICE candidate pair\n    gets selected. The pair contains both local and remote candidates being used\n    for a component. This event can be raised during a media session, if a new\n    pair of candidates with higher priority in the link are found.\n  </li>\n  <li><code>DataChannelOpen</code>: Raised when a data channel is open.</li>\n  <li><code>DataChannelClose</code>: Raised when a data channel is closed.</li>\n</ul>\n<p>\n  Registering to any of above events requires the application to provide a\n  callback function. Each event provides different information, so it is\n  recommended to consult the signature of the event listeners.\n</p>\n<p>\n  Flow control and congestion management is one of the most important features\n  of WebRTC. WebRTC connections start with the lowest bandwidth configured and\n  slowly ramps up to the maximum available bandwidth, or to the higher limit of\n  the exploration range in case no bandwidth limitation is detected. Notice that\n  WebRtcEndpoints in Kurento are designed in a way that multiple WebRTC\n  connections fed by the same stream share quality. When a new connection is\n  added, as it requires to start with low bandwidth, it will cause the rest of\n  connections to experience a transient period of degraded quality, until it\n  stabilizes its bitrate. This doesn't apply when transcoding is involved.\n  Transcoders will adjust their output bitrate based in bandwidth requirements,\n  but it won't affect the original stream. If an incoming WebRTC stream needs to\n  be transcoded, for whatever reason, all WebRtcEndpoints fed from transcoder\n  output will share a separate quality than the ones connected directly to the\n  original stream.\n</p>\n<p>\n  Note that the default <strong>VideoSendBandwidth</strong> range of the\n  endpoint is a VERY conservative one, and leads to a low maximum video quality.\n  Most applications will probably want to increase this to higher values such as\n  2000 kbps (2 mbps).\n</p>\n<p>\n  <strong>\n    Check the extended documentation of these parameters in\n    :rom:cls:`SdpEndpoint`, :rom:cls:`BaseRtpEndpoint`, and\n    :rom:cls:`RembParams`.\n  </strong>\n</p>\n<ul>\n  <li>\n    Input bandwidth: Values used to inform remote peers about the bitrate that\n    can be sent to this endpoint.\n    <ul>\n      <li>\n        <strong>MinVideoRecvBandwidth</strong>: Minimum input bitrate, requested\n        from WebRTC senders with REMB (Default: 30 Kbps).\n      </li>\n      <li>\n        <strong>MaxAudioRecvBandwidth</strong> and\n        <strong>MaxVideoRecvBandwidth</strong>: Maximum input bitrate, signaled\n        in SDP Offers to WebRTC and RTP senders (Default: unlimited).\n      </li>\n    </ul>\n  </li>\n  <li>\n    Output bandwidth: Values used to control bitrate of the video streams sent\n    to remote peers. It is important to keep in mind that pushed bitrate depends\n    on network and remote peer capabilities. Remote peers can also announce\n    bandwidth limitation in their SDPs (through the\n    <code>b={modifier}:{value}</code> attribute). Kurento will always enforce\n    bitrate limitations specified by the remote peer over internal\n    configurations.\n    <ul>\n      <li>\n        <strong>MinVideoSendBandwidth</strong>: REMB override of minimum bitrate\n        sent to WebRTC receivers (Default: 100 Kbps).\n      </li>\n      <li>\n        <strong>MaxVideoSendBandwidth</strong>: REMB override of maximum bitrate\n        sent to WebRTC receivers (Default: 500 Kbps).\n      </li>\n      <li>\n        <strong>RembParams.rembOnConnect</strong>: Initial local REMB bandwidth\n        estimation that gets propagated when a new endpoint is connected.\n      </li>\n    </ul>\n  </li>\n</ul>\n<p>\n  <strong>\n    All bandwidth control parameters must be changed before the SDP negotiation\n    takes place, and can't be changed afterwards.\n  </strong>\n</p>\n<p>\n  DataChannels allow other media elements that make use of the DataPad, to send\n  arbitrary data. For instance, if there is a filter that publishes event\n  information, it'll be sent to the remote peer through the channel. There is no\n  API available for programmers to make use of this feature in the\n  WebRtcElement. DataChannels can be configured to provide the following:\n</p>\n<ul>\n  <li>Reliable or partially reliable delivery of sent messages</li>\n  <li>In-order or out-of-order delivery of sent messages</li>\n</ul>\n<p>\n  Unreliable, out-of-order delivery is equivalent to raw UDP semantics. The\n  message may make it, or it may not, and order is not important. However, the\n  channel can be configured to be <i>partially reliable</i> by specifying the\n  maximum number of retransmissions or setting a time limit for retransmissions:\n  the WebRTC stack will handle the acknowledgments and timeouts.\n</p>\n<p>\n  The possibility to create DataChannels in a WebRtcEndpoint must be explicitly\n  enabled when creating the endpoint, as this feature is disabled by default. If\n  this is the case, they can be created invoking the createDataChannel method.\n  The arguments for this method, all of them optional, provide the necessary\n  configuration:\n</p>\n<ul>\n  <li>\n    <code>label</code>: assigns a label to the DataChannel. This can help\n    identify each possible channel separately.\n  </li>\n  <li>\n    <code>ordered</code>: specifies if the DataChannel guarantees order, which\n    is the default mode. If maxPacketLifetime and maxRetransmits have not been\n    set, this enables reliable mode.\n  </li>\n  <li>\n    <code>maxPacketLifeTime</code>: The time window in milliseconds, during\n    which transmissions and retransmissions may take place in unreliable mode.\n    This forces unreliable mode, even if <code>ordered</code> has been\n    activated.\n  </li>\n  <li>\n    <code>maxRetransmits</code>: maximum number of retransmissions that are\n    attempted in unreliable mode. This forces unreliable mode, even if\n    <code>ordered</code> has been activated.\n  </li>\n  <li>\n    <code>Protocol</code>: Name of the subprotocol used for data communication.\n  </li>\n</ul>\n      ",
      "extends": "BaseRtpEndpoint",
      "constructor": {
        "params": [
          {
            "name": "mediaPipeline",
            "doc": "the :rom:cls:`MediaPipeline` to which the endpoint belongs",
            "type": "MediaPipeline"
          },
          {
            "name": "recvonly",
            "doc": "Single direction, receive-only endpoint",
            "type": "boolean",
            "optional": true,
            "defaultValue": false
          },
          {
            "name": "sendonly",
            "doc": "Single direction, send-only endpoint",
            "type": "boolean",
            "optional": true,
            "defaultValue": false
          },
          {
            "name": "useDataChannels",
            "doc": "Activate data channels support",
            "type": "boolean",
            "optional": true,
            "defaultValue": false
          },
          {
            "name": "certificateKeyType",
            "doc": "Define the type of the certificate used in dtls",
            "type": "CertificateKeyType",
            "optional": true,
            "defaultValue": "RSA"
          },
          {
            "name": "qosDscp",
            "doc": "DSCP value to be used in network traffic sent from this endpoint\n              <p>\n              If this parameter is present with a value different to NO_VALUE, all traffic sent from this WebRTCEndpoint will be marked with the correspondent\n              DSCP value according to the DiffServ architecture. This may provide better handling of network traffic according to its characteristics associated to \n              its DSCP value. This better handling includes priority in forwarding traffic and probablity of packet drop in case of network congestion.\n              <p>\n              It must be taken into account that on Internet not all intermediate routers are guaranteed to enforce those DSCP values, even it is possible that \n              certain routers just block traffic marked with specific DSCP values, as discussed here https://datatracker.ietf.org/doc/html/rfc8835#section-4.2.\n              <p> \n              So, this feature must be managed with care and is mostly intended for private networks, where the network proprietor can configure and guarantee \n              DSCP management in all routers.",
            "type": "DSCPValue",
            "optional": true,
            "defaultValue": "NO_VALUE"
          }
        ],
        "doc": "Builder for the :rom:cls:`WebRtcEndpoint`"
      },
      "properties": [
        {
          "name": "networkInterfaces",
          "doc": "Local network interfaces used for ICE gathering.\n<p>\n  If you know which network interfaces should be used to perform ICE (for WebRTC\n  connectivity), you can define them here. Doing so has several advantages:\n</p>\n<ul>\n  <li>\n    The WebRTC ICE gathering process will be much quicker. Normally, it needs to\n    gather local candidates for all of the network interfaces, but this step can\n    be made faster if you limit it to only the interface that you know will\n    work.\n  </li>\n  <li>\n    It will ensure that the media server always decides to use the correct\n    network interface. With WebRTC ICE gathering it's possible that, under some\n    circumstances (in systems with virtual network interfaces such as\n    <code>docker0</code>) the ICE process ends up choosing the wrong local IP.\n  </li>\n</ul>\n<p>\n  <code>networkInterfaces</code> is a comma-separated list of network interface\n  names.\n</p>\n<p>Examples:</p>\n<ul>\n  <li><code>networkInterfaces=eth0</code></li>\n  <li><code>networkInterfaces=eth0,enp0s25</code></li>\n</ul>\n          ",
          "type": "String"
        },
        {
          "name": "iceTcp",
          "doc": "Enable ICE-TCP candidate gathering.\n<p>\n  This setting enables or disables using TCP for ICE candidate gathering in the\n  underlying libnice library:\n  https://libnice.freedesktop.org/libnice/NiceAgent.html#NiceAgent--ice-tcp\n</p>\n<p>\n  You might want to disable ICE-TCP to potentially speed up ICE gathering by\n  avoiding TCP candidates in scenarios where they are not needed.\n</p>\n<p><code>iceTcp</code> is either 1 (ON) or 0 (OFF). Default: 1 (ON).</p>\n          ",
          "type": "boolean"
        },
        {
          "name": "stunServerAddress",
          "doc": "STUN server IP address.\n<p>The ICE process uses STUN to punch holes through NAT firewalls.</p>\n<p>\n  <code>stunServerAddress</code> MUST be an IP address; domain names are NOT\n  supported.\n</p>\n<p>\n  You need to use a well-working STUN server. Use this to check if it works:<br />\n  https://webrtc.github.io/samples/src/content/peerconnection/trickle-ice/<br />\n  From that check, you should get at least one Server-Reflexive Candidate (type\n  <code>srflx</code>).\n</p>\n          ",
          "type": "String"
        },
        {
          "name": "stunServerPort",
          "doc": "Port of the STUN server",
          "type": "int"
        },
        {
          "name": "turnUrl",
          "doc": "TURN server URL.\n<p>\n  When STUN is not enough to open connections through some NAT firewalls, using\n  TURN is the remaining alternative.\n</p>\n<p>\n  Note that TURN is a superset of STUN, so you don't need to configure STUN if\n  you are using TURN.\n</p>\n<p>The provided URL should follow one of these formats:</p>\n<ul>\n  <li><code>user:password@ipaddress:port</code></li>\n  <li>\n    <code>user:password@ipaddress:port?transport=[udp|tcp|tls]</code>\n  </li>\n</ul>\n<p>\n  <code>ipaddress</code> MUST be an IP address; domain names are NOT supported.<br />\n  <code>transport</code> is OPTIONAL. Possible values: udp, tcp, tls. Default: udp.\n</p>\n<p>\n  You need to use a well-working TURN server. Use this to check if it works:<br />\n  https://webrtc.github.io/samples/src/content/peerconnection/trickle-ice/<br />\n  From that check, you should get at least one Server-Reflexive Candidate (type\n  <code>srflx</code>) AND one Relay Candidate (type <code>relay</code>).\n</p>\n          ",
          "type": "String"
        },
        {
          "name": "externalIPv4",
          "doc": "External IPv4 address of the media server.\n<p>\n  Forces all local IPv4 ICE candidates to have the given address. This is really\n  nothing more than a hack, but it's very effective to force a public IP address\n  when one is known in advance for the media server. In doing so, KMS will not\n  need a STUN or TURN server, but remote peers will still be able to contact it.\n</p>\n<p>\n  You can try using this setting if KMS is deployed on a publicly accessible\n  server, without NAT, and with a static public IP address. But if it doesn't\n  work for you, just go back to configuring a STUN or TURN server for ICE.\n</p>\n<p>\n  Only set this parameter if you know what you're doing, and you understand 100%\n  WHY you need it. For the majority of cases, you should just prefer to\n  configure a STUN or TURN server.\n</p>\n<p><code>externalIPv4</code> is a single IPv4 address.</p>\n<p>Example:</p>\n<ul>\n  <li><code>externalIPv4=198.51.100.1</code></li>\n</ul>\n          ",
          "type": "String"
        },
        {
          "name": "externalIPv6",
          "doc": "External IPv6 address of the media server.\n<p>\n  Forces all local IPv6 ICE candidates to have the given address. This is really\n  nothing more than a hack, but it's very effective to force a public IP address\n  when one is known in advance for the media server. In doing so, KMS will not\n  need a STUN or TURN server, but remote peers will still be able to contact it.\n</p>\n<p>\n  You can try using this setting if KMS is deployed on a publicly accessible\n  server, without NAT, and with a static public IP address. But if it doesn't\n  work for you, just go back to configuring a STUN or TURN server for ICE.\n</p>\n<p>\n  Only set this parameter if you know what you're doing, and you understand 100%\n  WHY you need it. For the majority of cases, you should just prefer to\n  configure a STUN or TURN server.\n</p>\n<p><code>externalIPv6</code> is a single IPv6 address.</p>\n<p>Example:</p>\n<ul>\n  <li><code>externalIPv6=2001:0db8:85a3:0000:0000:8a2e:0370:7334</code></li>\n</ul>\n          ",
          "type": "String"
        },
        {
          "name": "externalAddress",
          "doc": "External IP address of the media server.\n<p>\n  Forces all local IPv4 and IPv6 ICE candidates to have the given address. This\n  is really nothing more than a hack, but it's very effective to force a public\n  IP address when one is known in advance for the media server. In doing so, KMS\n  will not need a STUN or TURN server, but remote peers will still be able to\n  contact it.\n</p>\n<p>\n  You can try using this setting if KMS is deployed on a publicly accessible\n  server, without NAT, and with a static public IP address. But if it doesn't\n  work for you, just go back to configuring a STUN or TURN server for ICE.\n</p>\n<p>\n  Only set this parameter if you know what you're doing, and you understand 100%\n  WHY you need it. For the majority of cases, you should just prefer to\n  configure a STUN or TURN server.\n</p>\n<p><code>externalAddress</code> is a single IPv4 or IPv6 address.</p>\n<p>Examples:</p>\n<ul>\n  <li><code>externalAddress=198.51.100.1</code></li>\n  <li><code>externalAddress=2001:0db8:85a3:0000:0000:8a2e:0370:7334</code></li>\n</ul>\n@deprecated Use <code>externalIPv4</code> and/or <code>externalIPv6</code> instead.\n          ",
          "type": "String"
        },
        {
          "name": "ICECandidatePairs",
          "doc": "the ICE candidate pair (local and remote candidates) used by the ICE library for each stream.",
          "type": "IceCandidatePair[]",
          "readOnly": true
        },
        {
          "name": "IceConnectionState",
          "doc": "the ICE connection state for all the connections.",
          "type": "IceConnection[]",
          "readOnly": true
        }
      ],
      "methods": [
        {
          "params": [],
          "name": "gatherCandidates",
          "doc": "Start the ICE candidate gathering.\n<p>\n  This method triggers the asynchronous discovery of ICE candidates (as per the\n  Trickle ICE mechanism), and returns immediately. Every newly trickled\n  candidate is reported to the application by means of an\n  <code>IceCandidateFound</code> event. Finally, when all candidates have been\n  gathered, the <code>IceGatheringDone</code> event is emitted.\n</p>\n<p>\n  Normally, you would call this method as soon as possible after calling\n  <code>SdpEndpoint::generateOffer</code> or\n  <code>SdpEndpoint::processOffer</code>, to quickly start discovering\n  candidates and sending them to the remote peer.\n</p>\n<p>\n  You can also call this method <em>before</em> calling\n  <code>generateOffer</code> or <code>processOffer</code>. Doing so will include\n  any already gathered candidates into the resulting SDP. You can leverage this\n  behavior to implement fully traditional ICE (without Trickle): first call\n  <code>gatherCandidates</code>, then only handle the SDP messages after the\n  <code>IceGatheringDone</code> event has been received. This way, you're making\n  sure that all candidates have indeed been gathered, so the resulting SDP will\n  include all of them.\n</p>\n          "
        },
        {
          "params": [
            {
              "name": "candidate",
              "doc": "Remote ICE candidate",
              "type": "IceCandidate"
            }
          ],
          "name": "addIceCandidate",
          "doc": "Process an ICE candidate sent by the remote peer of the connection."
        },
        {
          "params": [
            {
              "name": "label",
              "doc": "Channel's label",
              "type": "String",
              "optional": true,
              "defaultValue": ""
            },
            {
              "name": "ordered",
              "doc": "If the data channel should guarantee order or not. If true, and maxPacketLifeTime and maxRetransmits have not been provided, reliable mode is activated.",
              "type": "boolean",
              "optional": true,
              "defaultValue": true
            },
            {
              "name": "maxPacketLifeTime",
              "doc": "The time window (in milliseconds) during which transmissions and retransmissions may take place in unreliable mode.\nNote that this forces unreliable mode, even if <code>ordered</code> has been activated.\n              ",
              "type": "int",
              "optional": true,
              "defaultValue": -1
            },
            {
              "name": "maxRetransmits",
              "doc": "maximum number of retransmissions that are attempted in unreliable mode.\nNote that this forces unreliable mode, even if <code>ordered</code> has been activated.\n              ",
              "type": "int",
              "optional": true,
              "defaultValue": -1
            },
            {
              "name": "protocol",
              "doc": "Name of the subprotocol used for data communication",
              "type": "String",
              "optional": true,
              "defaultValue": ""
            }
          ],
          "name": "createDataChannel",
          "doc": "Create a new data channel, if data channels are supported.\n<p>\n  Being supported means that the WebRtcEndpoint has been created with data\n  channel support, the client also supports data channels, and they have been\n  negotiated in the SDP exchange. Otherwise, the method throws an exception,\n  indicating that the operation is not possible.\n</p>\n<p>\n  Data channels can work in either unreliable mode (analogous to User Datagram\n  Protocol or UDP) or reliable mode (analogous to Transmission Control Protocol\n  or TCP). The two modes have a simple distinction:\n</p>\n<ul>\n  <li>\n    Reliable mode guarantees the transmission of messages and also the order in\n    which they are delivered. This takes extra overhead, thus potentially making\n    this mode slower.\n  </li>\n  <li>\n    Unreliable mode does not guarantee every message will get to the other side\n    nor what order they get there. This removes the overhead, allowing this mode\n    to work much faster.\n  </li>\n</ul>\n<p>If data channels are not supported, this method throws an exception.</p>\n          "
        },
        {
          "params": [
            {
              "name": "channelId",
              "doc": "The channel identifier",
              "type": "int"
            }
          ],
          "name": "closeDataChannel",
          "doc": "Closes an open data channel"
        }
      ],
      "events": [
        "OnIceCandidate",
        "IceCandidateFound",
        "OnIceGatheringDone",
        "IceGatheringDone",
        "OnIceComponentStateChanged",
        "IceComponentStateChange",
        "IceComponentStateChanged",
        "OnDataChannelOpened",
        "DataChannelOpen",
        "DataChannelOpened",
        "OnDataChannelClosed",
        "DataChannelClose",
        "DataChannelClosed",
        "NewCandidatePairSelected"
      ]
    }
  ],
  "complexTypes": [
    {
      "typeFormat": "ENUM",
      "values": [
        "NONE",
        "GENPTS",
        "FILL_IF_TRANSCODING"
      ],
      "name": "GapsFixMethod",
      "doc": "How to fix gaps when they are found in the recorded stream.\n<p>\nGaps are typically caused by packet loss in the input streams, such as when an\nRTP or WebRTC media flow suffers from network congestion and some packets don't\narrive at the media server.\n</p>\n<p>Different ways of handling gaps have different tradeoffs:</p>\n<ul>\n  <li>\n    <strong>NONE</strong>: Do not fix gaps.\n    <p>\n      Leave the stream as-is, and store it with any gaps that the stream might\n      have. Some players are clever enough to adapt to this during playback, so\n      that the gaps are reduced to a minimum and no problems are perceived by\n      the user; other players are not so sophisticated, and will struggle trying\n      to decode a file that contains gaps. For example, trying to play such a\n      file directly with Chrome will cause lipsync issues (audio and video will\n      fall out of sync).\n    </p>\n    <p>\n      This is the best choice if you need consistent durations across multiple\n      simultaneous recordings, or if you are anyway going to post-process the\n      recordings (e.g. with an extra FFmpeg step).\n    </p>\n    <p>\n      For example, assume a session length of 15 seconds: packets arrive\n      correctly during the first 5 seconds, then there is a gap, then data\n      arrives again for the last 5 seconds. Also, for simplicity, assume 1 frame\n      per second. With no fix for gaps, the RecorderEndpoint will store each\n      frame as-is, with these timestamps:\n    </p>\n    <pre>\n      frame 1  - 00:01\n      frame 2  - 00:02\n      frame 3  - 00:03\n      frame 4  - 00:04\n      frame 5  - 00:05\n      frame 11 - 00:11\n      frame 12 - 00:12\n      frame 13 - 00:13\n      frame 14 - 00:14\n      frame 15 - 00:15\n    </pre>\n    <p>\n      Notice how the frames between 6 to 10 are missing, but the last 5 frames\n      still conserve their original timestamp. The total length of the file is\n      detected as 15 seconds by most players, although playback could stutter or\n      hang during the missing section.\n    </p>\n  </li>\n  <li>\n    <strong>GENPTS</strong>: Adjust timestamps to generate a smooth progression\n    over all frames.\n    <p>\n      This technique rewrites the timestamp of all frames, so that gaps are\n      suppressed. It provides the best playback experience for recordings that\n      need to be played as-is (i.e. they won't be post-processed). However,\n      fixing timestamps might cause a change in the total duration of a file. So\n      different recordings from the same session might end up with slightly\n      different durations.\n    </p>\n    <p>\n      In our example, the RecorderEndpoint will change all timestamps that\n      follow a gap in the stream, and store each frame as follows:\n    </p>\n    <pre>\n      frame 1  - 00:01\n      frame 2  - 00:02\n      frame 3  - 00:03\n      frame 4  - 00:04\n      frame 5  - 00:05\n      frame 11 - 00:06\n      frame 12 - 00:07\n      frame 13 - 00:08\n      frame 14 - 00:09\n      frame 15 - 00:10\n    </pre>\n    <p>\n      Notice how the frames between 6 to 10 are missing, and the last 5 frames\n      have their timestamps corrected to provide a smooth increment over the\n      previous ones. The total length of the file is detected as 10 seconds, and\n      playback should be correct throughout the whole file.\n    </p>\n  </li>\n  <li>\n    <strong>FILL_IF_TRANSCODING</strong>: (NOT IMPLEMENTED YET).\n    <p>This is a proposal for future improvement of the RecorderEndpoint.</p>\n    <p>\n      It is possible to perform a dynamic adaptation of audio rate and add frame\n      duplication to the video, such that the missing parts are filled with\n      artificial data. This has the advantage of providing a smooth playback\n      result, and at the same time conserving all original timestamps.\n    </p>\n    <p>\n      However, the main issue with this method is that it requires accessing the\n      decoded media; i.e., transcoding must be active. For this reason, the\n      proposal is to offer this option to be enabled only when transcoding would\n      still happen anyways.\n    </p>\n    <p>\n      In our example, the RecorderEndpoint would change all missing frames like\n      this:\n    </p>\n    <pre>\n      frame 1  - 00:01\n      frame 2  - 00:02\n      frame 3  - 00:03\n      frame 4  - 00:04\n      frame 5  - 00:05\n      fake frame - 00:06\n      fake frame - 00:07\n      fake frame - 00:08\n      fake frame - 00:09\n      fake frame - 00:10\n      frame 11 - 00:11\n      frame 12 - 00:12\n      frame 13 - 00:13\n      frame 14 - 00:14\n      frame 15 - 00:15\n    </pre>\n    <p>\n      This joins the best of both worlds: on one hand, the playback should be\n      smooth and even the most basic players should be able to handle the\n      recording files without issue. On the other, the total length of the file\n      is left unmodified, so it matches with the expected duration of the\n      sessions that are being recorded.\n    </p>\n  </li>\n</ul>\n      "
    },
    {
      "typeFormat": "ENUM",
      "values": [
        "WEBM",
        "MKV",
        "MP4",
        "WEBM_VIDEO_ONLY",
        "WEBM_AUDIO_ONLY",
        "MKV_VIDEO_ONLY",
        "MKV_AUDIO_ONLY",
        "MP4_VIDEO_ONLY",
        "MP4_AUDIO_ONLY",
        "JPEG_VIDEO_ONLY",
        "KURENTO_SPLIT_RECORDER",
        "FLV"
      ],
      "name": "MediaProfileSpecType",
      "doc": "Media profile, used by the RecorderEndpoint builder to specify the codecs and media container that should be used for the recordings."
    },
    {
      "typeFormat": "REGISTER",
      "properties": [
        {
          "name": "isSeekable",
          "doc": "Seek is possible in video source",
          "type": "boolean"
        },
        {
          "name": "seekableInit",
          "doc": "First video position to do seek in ms",
          "type": "int64"
        },
        {
          "name": "seekableEnd",
          "doc": "Last video position to do seek in ms",
          "type": "int64"
        },
        {
          "name": "duration",
          "doc": "Video duration in ms",
          "type": "int64"
        }
      ],
      "name": "VideoInfo",
      "doc": ""
    },
    {
      "typeFormat": "REGISTER",
      "properties": [
        {
          "name": "candidate",
          "doc": "The candidate-attribute as defined in section 15.1 of ICE (rfc5245).",
          "type": "String"
        },
        {
          "name": "sdpMid",
          "doc": "If present, this contains the identifier of the 'media stream identification'.",
          "type": "String"
        },
        {
          "name": "sdpMLineIndex",
          "doc": "The index (starting at zero) of the m-line in the SDP this candidate is associated with.",
          "type": "int"
        }
      ],
      "name": "IceCandidate",
      "doc": "IceCandidate representation based on <code>RTCIceCandidate</code> interface.\n@see https://www.w3.org/TR/2018/CR-webrtc-20180927/#rtcicecandidate-interface"
    },
    {
      "typeFormat": "ENUM",
      "values": [
        "DISCONNECTED",
        "GATHERING",
        "CONNECTING",
        "CONNECTED",
        "READY",
        "FAILED"
      ],
      "name": "IceComponentState",
      "doc": "States of an ICE component."
    },
    {
      "typeFormat": "REGISTER",
      "properties": [
        {
          "name": "streamID",
          "doc": "Stream ID of the ICE connection.\n@deprecated Use <code>streamId</code> instead.\n          ",
          "type": "String"
        },
        {
          "name": "streamId",
          "doc": "Stream ID of the ICE connection",
          "type": "String"
        },
        {
          "name": "componentID",
          "doc": "Component ID of the ICE connection\n@deprecated Use <code>componentId</code> instead.\n          ",
          "type": "int"
        },
        {
          "name": "componentId",
          "doc": "Component ID of the ICE connection",
          "type": "int"
        },
        {
          "name": "localCandidate",
          "doc": "The local candidate used by the ICE library.",
          "type": "String"
        },
        {
          "name": "remoteCandidate",
          "doc": "The remote candidate used by the ICE library.",
          "type": "String"
        }
      ],
      "name": "IceCandidatePair",
      "doc": "The ICE candidate pair used by the ICE library, for a certain stream."
    },
    {
      "typeFormat": "REGISTER",
      "properties": [
        {
          "name": "streamId",
          "doc": "The ID of the stream",
          "type": "String"
        },
        {
          "name": "componentId",
          "doc": "The ID of the component",
          "type": "int"
        },
        {
          "name": "state",
          "doc": "The state of the component",
          "type": "IceComponentState"
        }
      ],
      "name": "IceConnection",
      "doc": "The ICE connection state for a certain stream and component."
    },
    {
      "typeFormat": "ENUM",
      "values": [
        "RSA",
        "ECDSA"
      ],
      "name": "CertificateKeyType",
      "doc": "."
    },
    {
      "typeFormat": "ENUM",
      "values": [
        "NO_DSCP",
        "NO_VALUE",
        "AUDIO_VERYLOW",
        "AUDIO_LOW",
        "AUDIO_MEDIUM",
        "AUDIO_HIGH",
        "VIDEO_VERYLOW",
        "VIDEO_LOW",
        "VIDEO_MEDIUM",
        "VIDEO_MEDIUM_THROUGHPUT",
        "VIDEO_HIGH",
        "VIDEO_HIGH_THROUGHPUT",
        "DATA_VERYLOW",
        "DATA_LOW",
        "DATA_MEDIUM",
        "DATA_HIGH",
        "CHROME_HIGH",
        "CHROME_MEDIUM",
        "CHROME_LOW",
        "CHROME_VERYLOW",
        "CS0",
        "CS1",
        "CS2",
        "CS3",
        "CS4",
        "CS5",
        "CS6",
        "CS7",
        "AF11",
        "AF12",
        "AF13",
        "AF21",
        "AF22",
        "AF23",
        "AF31",
        "AF32",
        "AF33",
        "AF41",
        "AF42",
        "AF43",
        "EF",
        "VOICEADMIT",
        "LE"
      ],
      "name": "DSCPValue",
      "doc": "Possible DSCP values \n      <p>\n      WebRTC recommended values are taken from RFC 8837 https://datatracker.ietf.org/doc/html/rfc8837#section-5 , These are the values from AUDIO_VERYLOW to DATA_HIGH. First element in the \n      name indicates kind of traffic that it should apply to, the second indicates relative priority. For video, a third field would indicate if the traffic is intended for high throughput or not.\n      As indicated on RFC 8837 section 5 diagram:\n\n          +=======================+==========+=====+============+============+\n          |       Flow Type       | Very Low | Low |   Medium   |    High    |\n          +=======================+==========+=====+============+============+\n          |         Audio         |  LE (1)  |  DF |  EF (46)   |  EF (46)   |\n          |                       |          | (0) |            |            |\n          +-----------------------+----------+-----+------------+------------+\n          +-----------------------+----------+-----+------------+------------+\n          |   Interactive Video   |  LE (1)  |  DF | AF42, AF43 | AF41, AF42 |\n          | with or without Audio |          | (0) |  (36, 38)  |  (34, 36)  |\n          +-----------------------+----------+-----+------------+------------+\n          +-----------------------+----------+-----+------------+------------+\n          | Non-Interactive Video |  LE (1)  |  DF | AF32, AF33 | AF31, AF32 |\n          | with or without Audio |          | (0) |  (28, 30)  |  (26, 28)  |\n          +-----------------------+----------+-----+------------+------------+\n          +-----------------------+----------+-----+------------+------------+\n          |          Data         |  LE (1)  |  DF |    AF11    |    AF21    |\n          |                       |          | (0) |            |            |\n          +-----------------------+----------+-----+------------+------------+\n\n      As indicated also in RFC, non interactive video is not considered\n      <p>\n      Apart from the WebRTC recommended values, we also include all possible values are referenced in http://www.iana.org/assignments/dscp-registry/dscp-registry.xml of course some of \n      those values are synonyms for the WebRTC recommended ones, they are included mainly for completeness\n      <p>\n      And as a last point, we include a shorthand for Chrome supported markings for low  (CS0), very low (CS1), medium (CS7) and high (CS7) priorities in priority property for RTCRtpSender parameters. See https://developer.mozilla.org/en-US/docs/Web/API/RTCRtpSender/setParameters\n      <p>\n      This only covers outgoing network packets from KMS, to complete the solution, DSCP must be also requested on client, unfortunately for traffic on the other direction, this must be requested to the \n      browser or client. On browser, the client application needs to use the following API https://www.w3.org/TR/webrtc-priority/"
    },
    {
      "typeFormat": "ENUM",
      "values": [
        "AES_128_CM_HMAC_SHA1_32",
        "AES_128_CM_HMAC_SHA1_80",
        "AES_256_CM_HMAC_SHA1_32",
        "AES_256_CM_HMAC_SHA1_80"
      ],
      "name": "CryptoSuite",
      "doc": "Describes the encryption and authentication algorithms"
    },
    {
      "typeFormat": "REGISTER",
      "properties": [
        {
          "name": "key",
          "doc": "<p>Master key and salt (plain text)</p>\n          <p>\n          This field provides the the cryptographic master key appended with the master salt, in plain text format. This allows to provide a key that is composed of readable ASCII characters.\n          </p>\n          <p>\n          The expected length of the key (as provided to this parameter) is determined by the crypto-suite for which the key applies (30 characters for AES_CM_128, 46 characters for AES_CM_256). If the length does not match the expected value, the key will be considered invalid.\n          </p>\n          <p>\n          If no key is provided, a random one will be generated using the `getrandom` system call.\n          </p>",
          "type": "String",
          "optional": true
        },
        {
          "name": "keyBase64",
          "doc": "<p>Master key and salt (base64 encoded)</p>\n          <p>\n          This field provides the cryptographic master key appended with the master salt, encoded in base64. This allows to provide a binary key that is not limited to the ASCII character set.\n          </p>\n          <p>\n          The expected length of the key (after being decoded from base64) is determined by the crypto-suite for which the key applies (30 bytes for AES_CM_128, 46 bytes for AES_CM_256). If the length does not match the expected value, the key will be considered invalid.\n          </p>\n          <p>\n          If no key is provided, a random one will be generated using the `getrandom` system call.\n          </p>",
          "type": "String",
          "optional": true
        },
        {
          "name": "crypto",
          "doc": "Selects the cryptographic suite to be used. For available values, please see the CryptoSuite enum.",
          "type": "CryptoSuite",
          "optional": true
        }
      ],
      "name": "SDES",
      "doc": "Security Descriptions for Media Streams"
    }
  ],
  "events": [
    {
      "properties": [],
      "extends": "Media",
      "name": "Recording",
      "doc": "Fired when the recoding effectively starts. ie: Media is received by the recorder, and <code>record()</code> method has been called."
    },
    {
      "properties": [],
      "extends": "Media",
      "name": "Paused",
      "doc": "@deprecated</br>Fired when the recorder goes to pause state"
    },
    {
      "properties": [],
      "extends": "Media",
      "name": "Stopped",
      "doc": "@deprecated</br>Fired when the recorder has been stopped and all the media has been written to storage."
    },
    {
      "properties": [
        {
          "name": "candidate",
          "doc": "New local candidate",
          "type": "IceCandidate"
        }
      ],
      "extends": "Media",
      "name": "OnIceCandidate",
      "doc": "Notifies a new local candidate.\nThese candidates should be sent to the remote peer, to complete the ICE negotiation process.\n@deprecated Use <code>IceCandidateFound</code> instead.\n      "
    },
    {
      "properties": [
        {
          "name": "candidate",
          "doc": "New local candidate",
          "type": "IceCandidate"
        }
      ],
      "extends": "Media",
      "name": "IceCandidateFound",
      "doc": "Notifies a new local candidate.\nThese candidates should be sent to the remote peer, to complete the ICE negotiation process.\n      "
    },
    {
      "properties": [],
      "extends": "Media",
      "name": "OnIceGatheringDone",
      "doc": "Event fired when al ICE candidates have been gathered.\n@deprecated Use <code>IceGatheringDone</code> instead.\n      "
    },
    {
      "properties": [],
      "extends": "Media",
      "name": "IceGatheringDone",
      "doc": "Event fired when al ICE candidates have been gathered."
    },
    {
      "properties": [
        {
          "name": "streamId",
          "doc": "The ID of the stream",
          "type": "int"
        },
        {
          "name": "componentId",
          "doc": "The ID of the component",
          "type": "int"
        },
        {
          "name": "state",
          "doc": "The state of the component",
          "type": "IceComponentState"
        }
      ],
      "extends": "Media",
      "name": "OnIceComponentStateChanged",
      "doc": "Event fired when and ICE component state changes.\nSee :rom:cls:`IceComponentState` for a list of possible states.\n@deprecated Use <code>IceComponentStateChanged</code> instead.\n      "
    },
    {
      "properties": [
        {
          "name": "streamId",
          "doc": "The ID of the stream",
          "type": "int"
        },
        {
          "name": "componentId",
          "doc": "The ID of the component",
          "type": "int"
        },
        {
          "name": "state",
          "doc": "The state of the component",
          "type": "IceComponentState"
        }
      ],
      "extends": "Media",
      "name": "IceComponentStateChange",
      "doc": "Event fired when and ICE component state changes.\nSee :rom:cls:`IceComponentState` for a list of possible states.\n@deprecated Use <code>IceComponentStateChanged</code> instead.\n      "
    },
    {
      "properties": [
        {
          "name": "streamId",
          "doc": "The ID of the stream",
          "type": "int"
        },
        {
          "name": "componentId",
          "doc": "The ID of the component",
          "type": "int"
        },
        {
          "name": "state",
          "doc": "The state of the component",
          "type": "IceComponentState"
        }
      ],
      "extends": "Media",
      "name": "IceComponentStateChanged",
      "doc": "Event fired when and ICE component state changes.\nSee :rom:cls:`IceComponentState` for a list of possible states.\n      "
    },
    {
      "properties": [
        {
          "name": "channelId",
          "doc": "The channel identifier",
          "type": "int"
        }
      ],
      "extends": "Media",
      "name": "OnDataChannelOpened",
      "doc": "Event fired when a new data channel is created.\n@deprecated Use <code>DataChannelOpened</code> instead.\n      "
    },
    {
      "properties": [
        {
          "name": "channelId",
          "doc": "The channel identifier",
          "type": "int"
        }
      ],
      "extends": "Media",
      "name": "DataChannelOpen",
      "doc": "Event fired when a new data channel is created.\n@deprecated Use <code>DataChannelOpened</code> instead.\n      "
    },
    {
      "properties": [
        {
          "name": "channelId",
          "doc": "The channel identifier",
          "type": "int"
        }
      ],
      "extends": "Media",
      "name": "DataChannelOpened",
      "doc": "Event fired when a new data channel is created."
    },
    {
      "properties": [
        {
          "name": "channelId",
          "doc": "The channel identifier",
          "type": "int"
        }
      ],
      "extends": "Media",
      "name": "OnDataChannelClosed",
      "doc": "Event fired when a data channel is closed.\n@deprecated Use <code>DataChannelClosed</code> instead.\n      "
    },
    {
      "properties": [
        {
          "name": "channelId",
          "doc": "The channel identifier",
          "type": "int"
        }
      ],
      "extends": "Media",
      "name": "DataChannelClose",
      "doc": "Event fired when a data channel is closed.\n@deprecated Use <code>DataChannelClosed</code> instead.\n      "
    },
    {
      "properties": [
        {
          "name": "channelId",
          "doc": "The channel identifier",
          "type": "int"
        }
      ],
      "extends": "Media",
      "name": "DataChannelClosed",
      "doc": "Event fired when a data channel is closed."
    },
    {
      "properties": [
        {
          "name": "candidatePair",
          "doc": "The new pair of candidates",
          "type": "IceCandidatePair"
        }
      ],
      "extends": "Media",
      "name": "NewCandidatePairSelected",
      "doc": "Event fired when a new pair of ICE candidates is used by the ICE library.\nThis could also happen in the middle of a session, though not likely.\n      "
    },
    {
      "properties": [
        {
          "name": "mediaType",
          "doc": "The media stream",
          "type": "MediaType"
        }
      ],
      "extends": "Media",
      "name": "OnKeySoftLimit",
      "doc": "Fired when encryption is used and any stream reached the soft key usage limit, which means it will expire soon."
    },
    {
      "properties": [],
      "extends": "Media",
      "name": "EndOfStream",
      "doc": "Event raised when the stream that the element sends out is finished."
    }
  ]
}
